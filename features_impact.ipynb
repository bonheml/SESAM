{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of image and text features on classification\n",
    "The trained models consistently showed worse or equivalent results when image or image + text was used for prediciton compared to text only. The aim of this section is to check if image and text are failing on the same predictions (i.e., images features are not helpful for sentiment analysis in memes) or if images can be more helpful than text in some cases (i.e., image and text features are complementary but images are less often used as a strong predictor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from src.utils.files import load_dfs, load_clfs\n",
    "from src.utils.embeddings import retrieve_all_embeds\n",
    "from src.utils.reports import generate_report\n",
    "from src.models.voting import soft_transform, hard_transform\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clfs, embeds, y_dev, voting=\"soft\", multitask=False, multilabel=False):\n",
    "    res = {}\n",
    "    if voting == \"soft\":\n",
    "        y_pred_dev = soft_transform(clfs, embeds[1], multilabel, multitask)\n",
    "        y_pred_test = soft_transform(clfs, embeds[2], multilabel, multitask)\n",
    "    else:\n",
    "        y_pred_dev = hard_transform(clfs, embeds[1], multilabel, multitask)\n",
    "        y_pred_test = hard_transform(clfs, embeds[2], multilabel, multitask)\n",
    "    if not multitask:\n",
    "        rep = classification_report(y_dev, y_pred_dev)\n",
    "        print(rep)\n",
    "    else:\n",
    "        rep = [classification_report(y_dev[:,col], y_pred_dev[:,col]) for col in range(y_dev.shape[1])]\n",
    "        cols = [\"Humour\", \"Sarcasm\", \"Offense\", \"Motivation\"]\n",
    "        for c, r in list(zip(cols, rep)):\n",
    "            print(\"results for class {}:\\n{}\".format(c, r))\n",
    "    res = {\"pred_cls_dev\": y_pred_dev, \"report_str\": rep, \"pred_cls_test\": y_pred_test}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_tasks(model_name, embeds, y_devs):\n",
    "    tasks = [\"task_a\", \"task_b\", \"task_c\"]\n",
    "    embed_type = [\"image_only\", \"text_only\", \"concatenated\"]\n",
    "    model_path = \"data/models/custom\"\n",
    "    res = []\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        print(task)\n",
    "        clf_names = [str(Path(\"{}/{}_{}_{}.joblib\".format(model_path,task, model_name, e)).resolve()) \n",
    "                     for e in embed_type]\n",
    "        clfs_task = [joblib.load(f) for f in clf_names]\n",
    "        multitask = True if i == 2 else False\n",
    "        multilabel = True if i ==1 else False\n",
    "        res.append(evaluate(clfs_task, embeds, y_devs[i], multitask=multitask, multilabel=multilabel)[\"pred_cls_test\"])\n",
    "    generate_report(*res, zipname=\"res_{}_ensemble.zip\".format(model_name))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev = load_dfs([\"data/train_cleaned_final.csv\", \"data/dev_cleaned_final.csv\"])\n",
    "cols = [\"Humour\", \"Sarcasm\", \"Offense\", \"Motivation\"]\n",
    "y_devs = [df_dev[\"Overall_sentiment\"].cat.codes,\n",
    "          df_dev[[\"Humour_bin\", \"Sarcasm_bin\", \"Offense_bin\", \"Motivation_bin\"]].to_numpy().astype(int),\n",
    "          pd.concat([df_dev[name].cat.codes for name in cols], axis=1).to_numpy()]\n",
    "embed = retrieve_all_embeds([(\"data/features/use.pkl.train\", \"data/features/xception.pkl.train\"), \n",
    "                              (\"data/features/use.pkl.dev\",\"data/features/xception.pkl.dev\"),\n",
    "                              (\"data/features/use.pkl.test\", \"data/features/xception.pkl.test\")])\n",
    "embed = list(zip(*embed.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        80\n",
      "           1       0.00      0.00      0.00       302\n",
      "           2       0.62      1.00      0.76       618\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.21      0.33      0.25      1000\n",
      "weighted avg       0.38      0.62      0.47      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87       773\n",
      "           1       0.75      1.00      0.86       751\n",
      "           2       0.60      1.00      0.75       601\n",
      "           3       0.00      0.00      0.00       366\n",
      "\n",
      "   micro avg       0.71      0.85      0.77      2491\n",
      "   macro avg       0.53      0.75      0.62      2491\n",
      "weighted avg       0.61      0.85      0.71      2491\n",
      " samples avg       0.71      0.80      0.72      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       227\n",
      "           1       0.34      0.94      0.50       343\n",
      "           2       0.50      0.09      0.16       341\n",
      "           3       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.35      1000\n",
      "   macro avg       0.21      0.26      0.17      1000\n",
      "weighted avg       0.29      0.35      0.23      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       249\n",
      "           1       0.49      1.00      0.66       491\n",
      "           2       0.00      0.00      0.00       214\n",
      "           3       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.49      1000\n",
      "   macro avg       0.12      0.25      0.16      1000\n",
      "weighted avg       0.24      0.49      0.32      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.83      0.53       399\n",
      "           1       0.35      0.13      0.19       352\n",
      "           2       0.00      0.00      0.00       220\n",
      "           3       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.38      1000\n",
      "   macro avg       0.18      0.24      0.18      1000\n",
      "weighted avg       0.28      0.38      0.28      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.78       634\n",
      "           1       0.00      0.00      0.00       366\n",
      "\n",
      "    accuracy                           0.63      1000\n",
      "   macro avg       0.32      0.50      0.39      1000\n",
      "weighted avg       0.40      0.63      0.49      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"lr\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.07      0.13        80\n",
      "           1       0.56      0.31      0.40       302\n",
      "           2       0.67      0.89      0.76       618\n",
      "\n",
      "    accuracy                           0.65      1000\n",
      "   macro avg       0.63      0.43      0.43      1000\n",
      "weighted avg       0.64      0.65      0.60      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       773\n",
      "           1       0.76      0.99      0.86       751\n",
      "           2       0.68      0.87      0.76       601\n",
      "           3       0.56      0.23      0.33       366\n",
      "\n",
      "   micro avg       0.74      0.85      0.79      2491\n",
      "   macro avg       0.70      0.77      0.71      2491\n",
      "weighted avg       0.72      0.85      0.76      2491\n",
      " samples avg       0.73      0.80      0.73      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.35      0.37       227\n",
      "           1       0.47      0.61      0.53       343\n",
      "           2       0.49      0.49      0.49       341\n",
      "           3       0.57      0.09      0.16        89\n",
      "\n",
      "    accuracy                           0.46      1000\n",
      "   macro avg       0.48      0.38      0.39      1000\n",
      "weighted avg       0.47      0.46      0.45      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.26      0.36       249\n",
      "           1       0.55      0.90      0.68       491\n",
      "           2       0.43      0.20      0.27       214\n",
      "           3       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.55      1000\n",
      "   macro avg       0.40      0.34      0.33      1000\n",
      "weighted avg       0.51      0.55      0.48      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.67      0.58       399\n",
      "           1       0.51      0.58      0.55       352\n",
      "           2       0.60      0.22      0.33       220\n",
      "           3       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.52      1000\n",
      "   macro avg       0.41      0.37      0.36      1000\n",
      "weighted avg       0.52      0.52      0.50      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.90      0.77       634\n",
      "           1       0.56      0.23      0.33       366\n",
      "\n",
      "    accuracy                           0.65      1000\n",
      "   macro avg       0.61      0.56      0.55      1000\n",
      "weighted avg       0.63      0.65      0.60      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"knn\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.61      0.14        80\n",
      "           1       0.27      0.15      0.19       302\n",
      "           2       0.60      0.20      0.30       618\n",
      "\n",
      "    accuracy                           0.22      1000\n",
      "   macro avg       0.32      0.32      0.21      1000\n",
      "weighted avg       0.46      0.22      0.25      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.24      0.36       773\n",
      "           1       0.73      0.24      0.36       751\n",
      "           2       0.61      0.19      0.29       601\n",
      "           3       0.38      0.80      0.51       366\n",
      "\n",
      "   micro avg       0.53      0.31      0.39      2491\n",
      "   macro avg       0.63      0.37      0.38      2491\n",
      "weighted avg       0.67      0.31      0.37      2491\n",
      " samples avg       0.45      0.28      0.31      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.58      0.37       227\n",
      "           1       0.38      0.10      0.15       343\n",
      "           2       0.36      0.12      0.18       341\n",
      "           3       0.11      0.40      0.17        89\n",
      "\n",
      "    accuracy                           0.24      1000\n",
      "   macro avg       0.28      0.30      0.22      1000\n",
      "weighted avg       0.32      0.24      0.22      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.63      0.37       249\n",
      "           1       0.47      0.13      0.20       491\n",
      "           2       0.20      0.10      0.14       214\n",
      "           3       0.10      0.35      0.16        46\n",
      "\n",
      "    accuracy                           0.26      1000\n",
      "   macro avg       0.26      0.30      0.22      1000\n",
      "weighted avg       0.34      0.26      0.23      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.73      0.53       399\n",
      "           1       0.37      0.09      0.15       352\n",
      "           2       0.24      0.12      0.16       220\n",
      "           3       0.07      0.24      0.11        29\n",
      "\n",
      "    accuracy                           0.36      1000\n",
      "   macro avg       0.27      0.30      0.24      1000\n",
      "weighted avg       0.35      0.36      0.30      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.24      0.36       634\n",
      "           1       0.38      0.80      0.51       366\n",
      "\n",
      "    accuracy                           0.45      1000\n",
      "   macro avg       0.52      0.52      0.43      1000\n",
      "weighted avg       0.56      0.45      0.41      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"gnb\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.01      0.02        80\n",
      "           1       0.00      0.00      0.00       302\n",
      "           2       0.62      1.00      0.76       618\n",
      "\n",
      "    accuracy                           0.62      1000\n",
      "   macro avg       0.37      0.34      0.26      1000\n",
      "weighted avg       0.42      0.62      0.47      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.99      0.87       773\n",
      "           1       0.75      0.99      0.86       751\n",
      "           2       0.63      0.89      0.74       601\n",
      "           3       0.57      0.17      0.26       366\n",
      "\n",
      "   micro avg       0.72      0.85      0.78      2491\n",
      "   macro avg       0.68      0.76      0.68      2491\n",
      "weighted avg       0.70      0.85      0.74      2491\n",
      " samples avg       0.72      0.80      0.72      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.89      0.37       227\n",
      "           1       0.00      0.00      0.00       343\n",
      "           2       0.00      0.00      0.00       341\n",
      "           3       0.19      0.26      0.22        89\n",
      "\n",
      "    accuracy                           0.23      1000\n",
      "   macro avg       0.10      0.29      0.15      1000\n",
      "weighted avg       0.07      0.23      0.10      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.97      0.41       249\n",
      "           1       0.00      0.00      0.00       491\n",
      "           2       0.00      0.00      0.00       214\n",
      "           3       0.12      0.20      0.15        46\n",
      "\n",
      "    accuracy                           0.25      1000\n",
      "   macro avg       0.10      0.29      0.14      1000\n",
      "weighted avg       0.07      0.25      0.11      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57       399\n",
      "           1       0.00      0.00      0.00       352\n",
      "           2       0.00      0.00      0.00       220\n",
      "           3       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.40      1000\n",
      "   macro avg       0.10      0.25      0.14      1000\n",
      "weighted avg       0.16      0.40      0.23      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.93      0.77       634\n",
      "           1       0.57      0.17      0.26       366\n",
      "\n",
      "    accuracy                           0.65      1000\n",
      "   macro avg       0.61      0.55      0.51      1000\n",
      "weighted avg       0.63      0.65      0.58      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"abc\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.61      0.71        80\n",
      "           1       0.77      0.70      0.73       302\n",
      "           2       0.85      0.91      0.88       618\n",
      "\n",
      "    accuracy                           0.82      1000\n",
      "   macro avg       0.82      0.74      0.77      1000\n",
      "weighted avg       0.82      0.82      0.82      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       773\n",
      "           1       0.89      0.97      0.93       751\n",
      "           2       0.85      0.88      0.87       601\n",
      "           3       0.83      0.74      0.78       366\n",
      "\n",
      "   micro avg       0.88      0.91      0.89      2491\n",
      "   macro avg       0.87      0.89      0.88      2491\n",
      "weighted avg       0.88      0.91      0.89      2491\n",
      " samples avg       0.83      0.85      0.82      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.74      0.78       227\n",
      "           1       0.73      0.81      0.77       343\n",
      "           2       0.75      0.77      0.76       341\n",
      "           3       0.88      0.69      0.77        89\n",
      "\n",
      "    accuracy                           0.77      1000\n",
      "   macro avg       0.80      0.75      0.77      1000\n",
      "weighted avg       0.77      0.77      0.77      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.64      0.73       249\n",
      "           1       0.75      0.91      0.82       491\n",
      "           2       0.78      0.66      0.72       214\n",
      "           3       1.00      0.67      0.81        46\n",
      "\n",
      "    accuracy                           0.78      1000\n",
      "   macro avg       0.84      0.72      0.77      1000\n",
      "weighted avg       0.79      0.78      0.78      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81       399\n",
      "           1       0.73      0.84      0.78       352\n",
      "           2       0.83      0.77      0.80       220\n",
      "           3       1.00      0.45      0.62        29\n",
      "\n",
      "    accuracy                           0.79      1000\n",
      "   macro avg       0.85      0.71      0.75      1000\n",
      "weighted avg       0.80      0.79      0.79      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89       634\n",
      "           1       0.85      0.73      0.79       366\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.85      0.83      0.84      1000\n",
      "weighted avg       0.86      0.86      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"mlp\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_a\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.15      0.25        80\n",
      "           1       0.75      0.36      0.48       302\n",
      "           2       0.71      0.96      0.82       618\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.72      0.49      0.52      1000\n",
      "weighted avg       0.72      0.71      0.67      1000\n",
      "\n",
      "task_b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lb732/Projects/memotion_analysis/.venv/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89       773\n",
      "           1       0.78      0.99      0.87       751\n",
      "           2       0.72      0.94      0.82       601\n",
      "           3       0.82      0.39      0.53       366\n",
      "\n",
      "   micro avg       0.78      0.89      0.83      2491\n",
      "   macro avg       0.78      0.83      0.78      2491\n",
      "weighted avg       0.78      0.89      0.81      2491\n",
      " samples avg       0.76      0.83      0.77      2491\n",
      "\n",
      "task_c\n",
      "results for class Humour:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.64       227\n",
      "           1       0.66      0.75      0.70       343\n",
      "           2       0.68      0.70      0.69       341\n",
      "           3       0.62      0.27      0.37        89\n",
      "\n",
      "    accuracy                           0.66      1000\n",
      "   macro avg       0.65      0.59      0.60      1000\n",
      "weighted avg       0.66      0.66      0.65      1000\n",
      "\n",
      "results for class Sarcasm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.45      0.53       249\n",
      "           1       0.64      0.91      0.75       491\n",
      "           2       0.59      0.32      0.42       214\n",
      "           3       0.53      0.17      0.26        46\n",
      "\n",
      "    accuracy                           0.64      1000\n",
      "   macro avg       0.61      0.46      0.49      1000\n",
      "weighted avg       0.63      0.64      0.60      1000\n",
      "\n",
      "results for class Offense:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.71       399\n",
      "           1       0.67      0.67      0.67       352\n",
      "           2       0.67      0.41      0.51       220\n",
      "           3       0.67      0.14      0.23        29\n",
      "\n",
      "    accuracy                           0.65      1000\n",
      "   macro avg       0.66      0.51      0.53      1000\n",
      "weighted avg       0.65      0.65      0.64      1000\n",
      "\n",
      "results for class Motivation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.95      0.83       634\n",
      "           1       0.82      0.39      0.53       366\n",
      "\n",
      "    accuracy                           0.74      1000\n",
      "   macro avg       0.78      0.67      0.68      1000\n",
      "weighted avg       0.76      0.74      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = evaluate_all_tasks(\"rf\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
