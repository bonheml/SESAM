{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of image and text features on classification\n",
    "The trained models consistently showed worse or equivalent results when image or image + text was used for prediciton compared to text only. The aim of this section is to check if image and text are failing on the same predictions (i.e., images features are not helpful for sentiment analysis in memes) or if images can be more helpful than text in some cases (i.e., image and text features are complementary but images are less often used as a strong predictor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "from src.utils.files import load_dfs, load_clfs\n",
    "from src.utils.embeddings import retrieve_all_embeds\n",
    "from src.utils.reports import generate_reports\n",
    "from src.models.voting import soft_transform, hard_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clfs, embeds, y_dev, voting=\"soft\", multitask=False):\n",
    "    res = {}\n",
    "    if voting == \"soft\":\n",
    "        y_pred_dev = soft_transform(clfs, embed[1])\n",
    "        y_pred_test = soft_transform(clf, embed[2])\n",
    "    else:\n",
    "        y_pred_dev = hard_transform(clfs, embed[1])\n",
    "        y_pred_test = hard_transform(clf, embed[2])\n",
    "    if not multitask:\n",
    "        rep = classification_report(y_dev, y_pred_dev)\n",
    "        print(rep)\n",
    "    else:\n",
    "        rep = [classification_report(y_dev[:,col], y_pred_dev[:,col]) for col in range(y_dev.shape[1])]\n",
    "        cols = [\"Humour\", \"Sarcasm\", \"Offense\", \"Motivation\"]\n",
    "        for c, r in list(zip(cols, rep)):\n",
    "            print(\"results for class {}:\\n{}\".format(c, r))\n",
    "    res[item] = {\"pred_cls_dev\": y_pred_dev, \"report_str\": rep, \"pred_cls_test\": y_pred_test}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_tasks(model_name, embeds, y_devs):\n",
    "    tasks = [\"task_a\", \"task_b\", \"task_c\"]\n",
    "    embed_type = [\"text_only\", \"image_only\", \"concatenated\"]\n",
    "    model_path = \"data/models/custom\"\n",
    "    res = []\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        clf_names = [\"{}/{}_{}_{}.joblib\".format(model_path,task, model_name, e) for e in embed_type]\n",
    "        clfs_task = load_clfs(clf_names)\n",
    "        multitask = True if i == 2 else False\n",
    "        res.append(evaluate(clfs_task, embeds, y_dev, multitask=True))\n",
    "    generate_reports(*res, model_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_dev = load_dfs([\"data/train_cleaned_final.csv\", \"data/dev_cleaned_final.csv\"])\n",
    "cols = [\"Humour\", \"Sarcasm\", \"Offense\", \"Motivation\"]\n",
    "y_devs = [df_dev[\"Overall_sentiment\"].cat.codes,\n",
    "          df_dev[[\"Humour_bin\", \"Sarcasm_bin\", \"Offense_bin\", \"Motivation_bin\"]].to_numpy().astype(int),\n",
    "          pd.concat([df_dev[name].cat.codes for name in cols], axis=1).to_numpy()]\n",
    "embed = retrieve_all_embeds([(\"data/features/use.pkl.train\", \"data/features/xception.pkl.train\"), \n",
    "                              (\"data/features/use.pkl.dev\",\"data/features/xception.pkl.dev\"),\n",
    "                              (\"data/features/use.pkl.test\", \"data/features/xception.pkl.test\")])\n",
    "embed = list(zip(*embed.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"lr\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"knn\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"gnb\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"abc\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"mlp\", embed, y_devs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_all_tasks(\"rf\", embed, y_devs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
